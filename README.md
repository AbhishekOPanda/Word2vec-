# Word2vec
word2vec  embeddings  to  find  the  nearest  words  for  a  given  word.  The 
pretrained word embeddings file (vectors.txt) is already given, so you do not need to run word2vec model. 
The file contains word2vec embeddings for 400K words, and the dimension of each vector is 50. Each line 
contains  the  word and  its  corresponding  vector.  The  first  word in each line  is  the  word,  followed  by  50 
numbers, where each number is a dimension of the vector. 
1) Semantics: Use the pre-trained embeddings file to compute the 20 most similar words using cosine 
similarity for the following words, and show your work.   
a. life  
b. market 
c. Stanford 
  
2) Visualization  
a. Create a t-sne visualization, displaying all the words in the file.  
b. Use t-sne visualization to display the nearest 20 words for a given word. Create a separate 
visualization of all the 3 words given in question 1), where each visualization displays the nearest 
20 words for a word.  
